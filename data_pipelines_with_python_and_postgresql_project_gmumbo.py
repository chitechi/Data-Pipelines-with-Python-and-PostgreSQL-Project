# -*- coding: utf-8 -*-
"""Data Pipelines with Python and PostgreSQL Project_GMumbo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FnZ356kwse4FdO0PhuOpIZb-wKNXw7Yq

##Predicting Equipment Failure and Scheduling Maintenance Proactively

#Project Deliverable

● A GitHub repository with a python file (.py) with your solution.
Problem Statement

Equipment failure is a major cause of downtime in the telecommunications industry, which can
result in significant financial losses and customer dissatisfaction. To minimize downtime and
ensure optimal performance, it is crucial to identify potential equipment failures and schedule
maintenance accordingly proactively. This requires the collection and analysis of large amounts
of data generated by various equipment and network sensors.
The deliverable for this project is a data pipeline that can efficiently collect, clean, and analyze
equipment and network sensor data. The pipeline should be designed to identify potential
equipment failures and schedule maintenance proactively, minimizing downtime and improving
overall equipment performance. The data pipeline will be built using Python and PostgreSQL
and with the Postgres database hosted on Google Cloud.
Guidelines

Here are some guidelines and hints to help you create the data pipeline:

● Data Extraction: The data pipeline should be designed to collect data from various
sources, including network sensors, equipment sensors, and maintenance records.
Sample datasets for data extraction will be provided by the client and should be used for
building the pipeline.

● Data Transformation: The collected data must be cleaned and transformed to ensure
consistency and quality. This will involve removing duplicates, fixing missing data, and
normalizing the data for consistency. You can also explore the following techniques:

○ Aggregation: Summarizing data into useful metrics such as the total number of
equipment failures, average time between failures, etc.

○ Joining: Combining multiple datasets based on common fields or keys to create a
unified view of the data.

○ Data enrichment: Combining internal data with external data sources such as
weather data or other publicly available datasets to gain additional insights.

● Data Analysis: The cleaned data will be used to build machine learning models that can
predict potential equipment failures and schedule maintenance proactively. The models
will be designed to analyze equipment and network sensor data in real time to identify
anomalies and predict potential failures. You don’t need to implement this step in the
data pipeline.

● Data Loading: The resulting data will be stored in a PostgreSQL database.
Sample Datasets for Data Extraction
Sample datasets (https://bit.ly/3YNdO2Y) will be provided by the client for data extraction. The
datasets will include equipment sensor data, network sensor data, and maintenance records.

The datasets will be in CSV format and will include the following fields:

● Equipment sensor data: ID, date, time, sensor reading
● Network sensor data: ID, date, time, sensor reading
● Maintenance records: ID, date, time, equipment ID, maintenance type
"""

!pip install psycopg2-binary

import pandas as pd
from sqlalchemy import create_engine

# Define the Google Cloud SQL database connection
POSTGRES_ADDRESS = '34.237.226.14'
POSTGRES_PORT = '5437'
POSTGRES_USERNAME = 'postgres'
POSTGRES_PASSWORD = 'password_01'
POSTGRES_DBNAME = 'telecoms_data'

"""database engine"""

# Define the database engine
postgres_str = ('postgresql://{username}:{password}@{ipaddress}:{port}/{dbname}'
                .format(username=POSTGRES_USERNAME,
                        password=POSTGRES_PASSWORD,
                        ipaddress=POSTGRES_ADDRESS,
                        port=POSTGRES_PORT,
                        dbname=POSTGRES_DBNAME))
engine = create_engine(postgres_str)

"""##Data Extraction"""

def extract_data():
    # Load the equipment sensor data
    df_equipment_sensor = pd.read_csv('equipment_data.csv')

    # Load the network sensor data
    df_network_sensor = pd.read_csv('network_data.csv')

    # Load the maintenance records data
    df_maintenance_sensor = pd.read_csv('maintenance_records.csv')

    return df_equipment_sensor, df_network_sensor, df_maintenance_sensor

"""##Data Transformation"""

def transform_data(df_equipment_sensor, df_network_sensor, df_maintenance_sensor):
    # Remove duplicates
    df_equipment_sensor.drop_duplicates(inplace=True)
    df_network_sensor.drop_duplicates(inplace=True)
    df_maintenance_sensor.drop_duplicates(inplace=True)

    # Fix missing data
    df_equipment_sensor.fillna(method='ffill', inplace=True)
    df_network_sensor.fillna(method='ffill', inplace=True)
    df_maintenance_sensor.fillna(method='ffill', inplace=True)

    # Normalize the data for consistency
    df_equipment_sensor['date_time'] = pd.to_datetime(df_equipment_sensor['date'] + ' ' + df_equipment_sensor['time'])
    df_equipment_sensor.drop(['date', 'time'], axis=1, inplace=True)

    df_network_sensor['date_time'] = pd.to_datetime(df_network_sensor['date'] + ' ' + df_network_sensor['time'])
    df_network_sensor.drop(['date', 'time'], axis=1, inplace=True)

    df_maintenance_sensor['date_time'] = pd.to_datetime(df_maintenance_sensor['date'] + ' ' + df_maintenance_sensor['time'])
    df_maintenance_sensor.drop(['date', 'time'], axis=1, inplace=True)

    # Aggregate the data
    equipment_summary = df_equipment_sensor.groupby('ID').agg({'date_time': ['min', 'max'], 'sensor_reading': ['mean', 'max']})
    equipment_summary.columns = ['first_seen', 'last_seen', 'average_reading', 'max_reading']
    network_summary = df_network_sensor.groupby('ID').agg({'date_time': ['min', 'max'], 'sensor_reading': ['mean', 'max']})
    network_summary.columns = ['first_seen', 'last_seen', 'average_reading', 'max_reading']

    # Join the data
    sensor_summary = pd.merge(equipment_summary, network_summary, how='outer', left_index=True, right_index=True)
    sensor_summary = sensor_summary.reset_index()
    sensor_summary = sensor_summary.rename(columns={'ID': 'equipment_ID'})

    df_maintenance_sensor = df_maintenance_sensor[['date_time', 'equipment_ID', 'maintenance_type']]

    return sensor_summary, df_maintenance_sensor

"""##Data Loading and Defining the main function"""

def load_data(sensor_summary, maintenance_df):
    # Load the data to PostgreSQL
    sensor_summary.to_sql('sensor_summary', engine, if_exists='replace')
    maintenance_df.to_sql('maintenance_records', engine, if_exists='replace')

def main():
    df_equipment_sensor, df_network_sensor, df_maintenance_sensor = extract_data()
    sensor_summary, maintenance_df = transform_data(df_equipment_sensor, df_network_sensor, df_maintenance_sensor)
    load_data(sensor_summary, maintenance_df)

if __name__ == '__main__':
    main()